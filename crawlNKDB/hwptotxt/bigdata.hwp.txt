



『통일교육 선도대학 지정․육성사업』
통일빅데이터센터 연구개발 보고서





2019. 12.









한동대학교
통일교육선도대학사업단

<제목 차례>
   요약5
Ⅰ. 통일빅데이터센터 연구 개요 (박지현)7
1-1. 통일빅데이터센터 연구 배경7
1-2. 통일빅데이터센터 연구 목적9
1-3. 통일빅데이터센터 연구의 방법 및 내용9
1-4. 통일빅데이터센터 연구 결과물의 활용방안 및 기대효과11
1-5. 통일빅데이터센터 연구개발 보고서 구성 소개12
Ⅱ. 통일 및 북한 관련 자료 발간 기관 및 발간물 현황 (연제린)13
2-1. 통일 및 북한 관련 자료 발간 기관 현황13
2-2. 통일 및 북한 관련 발간물 현황13
2-3. 통일 및 북한 관련 자료 검색 및 분석 서비스 현황13
Ⅲ. 통일빅데이터센터 기획15
3-1. 통일빅데이터센터 개요 (강신범)15
3-2. 통일빅데이터센터 홈페이지 전체 구조 (한명천)15
3-3. 검색 분야 기획 상세 (한명천)15
3-4. 분석 분야 기획 상세 (강신범)16
3-5. 추천 분야 (주요 이벤트 분석) 기획 상세 (한명천, 연제린)16
3-6. 홈페이지 부가 기능 (한명천)16
Ⅳ. 통일빅데이터센터 구축 (서상원)17
4-1. 웹프레임워크 기술 일반17
4-2. 데이터 시각화 기술 일반17
4-3. 통일빅데이터센터 기술 설계17
4-4. 프로토타입18
Ⅴ. 통일빅데이터센터 자료 검색 방법론 (원은지)19
5-1. 웹기반 자료 검색 방법론 일반 19
5-2. 통일 빅데이터센터 자료 검색 엔진의 특수성 19
5-3. 통일 빅데이터센터 자료 검색 시스템 아키텍쳐 19
5-4. 프로토타입20
5-5. 자료 검색 시스템 구축 현황20
Ⅵ. 부록 (수집 자료)21
6-1. 통일 및 북한 관련 국내 기관 목록 (연제린)21
6-2. 통일 및 북한 자료 관련 국내 기관 발간물 목록21
6-3. 통일 및 북한 자료 관련 국내 포털 사이트 목록 (연제린)21

<표 차례>
표 1  통일 및 북한 관련 자료 분류16

<그림 차례>
그림 1 통일빅데이터센터 홈페이지 시스템 아키텍쳐18
그림 2 통일빅데이터센터 자료검색 시스템 아키텍쳐19

<수식 차례>


Ⅴ. 통일빅데이터센터 자료 검색 방법론 (원은지)

5-1. 웹기반 자료 검색 방법론 일반 
본 절에서는 검색 엔진, 검색 엔진 종류 및 기법, 검색 엔진 파이프라인에 대해 소개하며 일반적인 웹기반 자료 검색 방법론에 대해서 설명한다. 

5-1-1. 검색 엔진이란
검색 엔진은 컴퓨터 시스템에 저장된 정보를 찾아주는 것을 도와주도록 설계된 정보 검색 시스템이다. 검색 엔진은 일종의 데이타베이스로 인터넷의 각종 사이트에 대한 정보를 보유하고 있다. 검색 엔진을 통해 사용자는 필요한 정보를 찾기 위해 일일이 인터넷을 돌아다니지 않고도 원하는 정보를 찾을 수 있게 된다. 

검색 엔진의 정보를 구축하기 위해서는 로봇, 에이전트, 스파이더, 웜, 크롤러 등의 정보수집 프로그램을 이용한다. WWW World-Wide Web이란 광범위한 문서들로의 편리한 접근 방법을 제공해주는 분산 하이퍼미디어 정보 검색 시스템이다.
에 있는 정보도 이와 같은 정보수집 프로그램을 통해 수집된다. 검색 엔진은 키워드 검색에 사용할 수 있도록 웹문서의 내용을 색인화 하는 역할을 수행한다. 이렇게 색인화된 정보는 사용자가 원하는 것을 찾아볼 수 있도록 데이터베이스로 구축된다. 

5-1-2. 검색 엔진 종류
검색 엔진의 종류는 크게 주제별 디렉토리, 검색어 입력, 통합 검색으로 나눌 수 있다. 각 검색 엔진의 종류에 대한 설명과 장단점을 살펴보겠다. 

1) 주제별 디렉토리
주제별 디렉토리 검색 엔진은 특정 주제별로 각 페이지들을 분류하여 정리해 놓는다. 이 방식은 사용자가 찾고자하는 정보에 대한 지식이 분명하지 않거나, 어떤 분야에 대한 검색을 원할 경우에 적합한 검색 엔진이다. 보통 주제별 디렉토리 검색 엔진은 큰 주제에서 작은 주제로 점차적으로 좁혀지며 검색하는 방식을 기원한다. 

이 방식의 장점은 찾고자 하는 것에 대해 아무런 지식이 없어도 원하는 것의 대분류 정도만 알아도 찾을 수 있다. 하지만 원하는 정보를 얻기까지 여러 단계를 거쳐야 하므로 중간에 주제별 디렉토리를 잘못 들어가면 관련된 정보를 찾을 수 없게 된다는 명확한 단점이 존재한다. 

2) 검색어 입력
검색어 입력 검색 엔진은 하나의 데이터베이스에 모든 URL을 저장하고 특정 키워드를 입력함으로써 원하는 정보를 찾는 방식이다. 이 방식은 모든 검색 엔진에서 지원하고 있는 형태로 현재 가장 널리 사용되고 있다. 사용자가 찾고자 하는 정보의 단어를 입력하여 검색 엔진이 그 단어에 대하여 만족하는 검색 내용을 보여준다. 

이 방식은 사용자가 찾고자 하는 검색 대상의 단어를 입력하면 곧바로 그에 해당하는 결과를 찾아준다는 장점이 있다. 하지만 검색어의 단어가 정확하지 않다면 사용자가 원하는 정보를 빠른 시간 내에 찾을 수 없을 뿐만 아니라 전혀 다른 검색 결과를 보여주게 된다. 또한 이는 사전의 단어를 찾는 방식으므로 검색 결과가 잘 나오려면 검색 엔진 안에 많은 정보를 가지고 있어야 한다.

3) 통합 검색
통합 검색 엔진은 한번의 검색으로 여러 검색 엔진을 동시에 검색한다. 대부분 멀티스레드 기법을 사용해서 여러 곳을 검색할 수 있게 한다. 인터넷의 비약적인 발전으로 엄청난 정보들이 인터넷에 모이게 되었다. 그로인해 여러 검색 엔진이 등장하게 되었다. 통합 검색은 여러 검색 엔진을 통합한 검색 엔진으로 각각 검색 엔진의 특성을 구분하여 검색을 하던 주제별 디렉토리 검색 엔진과 검색어 입력 검색 엔진을 따로 찾아 다닐 필요가 없는 것이 특징이다.

통합 검색 엔진은 한번의 키워드 입력으로 다양한 검색 엔진을 참조함으로 간편하게 정보를 찾을 수 있으며 다량의 자료를 찾을 수 있다는 장점이 있다. 하지만 여러 검색 엔진을 검색하기 때문에 다소 시간이 걸리며 너무 많은 정보의 출력으로 원하는 정보를 가려내기 어렵다는 단점이 있다.  


5-1-3. 검색 엔진 파이프라인
본 절에서는 검색 엔진의 파이프라인인 크롤링, 인덱싱, 순위 결정, 쿼리 프로세싱을 순서대로 설명하면서 검색 엔진이 작동되는 원리를 살펴볼 것이다. 현재 대부분의 검색 엔진은 다음과 같이 일반적인 파이프라인을 거치며 사용자의 검색 쿼리/키워드에 가장 적합한 결과물을 제공해준다. 검색 엔진이 가장 적합한 결과물을 사용자에게 보여주기 위해 가장 먼저 인터넷 상의 수많은 문서들을 수집한다. 그 다음 절차로 수집한 데이터를 검색 엔진이 사용하기 쉽게 정리 및 분류하여 검색 엔진 데이터베이스에 인덱싱 (색인) 해둔다. 마지막으로 사용자가 검색어를 입력했을 때 인덱싱해둔 데이터를 사용하여 입력된 검색어에 가장 적합한 문서들의 순위를 매겨 사용자에게 보여준다.  
 
1) 크롤링
크롤링은 문서를 수집하는 작업을 말한다. 검색 엔진 크롤러 crawler, bot 또는 spider 라고 부르기도 하며 정보를 수집하는 프로그램이다.
가 인터넷 상의 수많은 사이트들을 찾아내어 수집하는 역할을 한다. 이때 크롤러는 자신이 방문한 페이지에 걸려 있는 모든 링크들까지 확장해서 정보를 모은다. 또한 이미 방문했거나 수집되어 있는 정보라도 일정 기간을 정해 지속적으로 재방문해 정보의 변화 여부를 확인하여 최신 상태로 업데이트를 한다. 크롤러는 웹페이지에 있는 텍스트, 사진, 영상, URL 등 다양한 정보를 수집해 검색이 이루어질 수 있도록 데이터베이스에 저장한다. 

크롤러가 빠짐없이 정보를 수집하고 정보를 최신 상태로 유지하기 위해서는 URL 관리가 중요하다. 크롤러는 내부에 방문한 URL 리스트와 방문해야 할 URL 리스트를 가지고 있어서, 신규 URL이 들어오면 이 리스트들과 비교해 중복 여부를 체크하고 중복되지 않은 URL이면 방문할 리스트에 넣어 크롤러가 이후에 방문해 정보를 수집한다. 

2) 인덱싱
검색 엔진 인덱싱은 빠르고 정확한 정보 검색이 용이하도록 데이터를 수집, 파싱 그리고 저장한다. 인덱스 디자인은 언어학, 인지 심리학, 수학, 정보학, 컴퓨터 과학의 학문간 개념을 통합한다. 인기 있는 검색 엔진은 온라인에서의 자연어 문서의 전체 텍스트 인덱싱에 초점을 맞춘다. 비디오, 오디오, 그래픽과 같은 미디어 유형도 검색할 수 있다. 
 
인덱스를 저장하는 목적은 검색 쿼리에 대한 관련 문서를 찾을 때 속도와 성능을 최적화하기 위한 것이다. 인덱스가 없으면 검색 엔진은 상당한 시간과 계산 능력이 필요한 말뭉치의 모든 문서를 스캔하게 된다. 
 
검색 엔진 아키텍처는 인덱싱이 수행되는 방식과 인덱스 저장 방법에 따라 다양하다. 그중 역인덱싱은 단어 또는 숫자가 어떤 문서에 속해있는지와 같은 정보들을 인덱스로 저장하는 방식을 취한다. 이 역인덱싱의 목적은 문서가 데이터베이스에 추가될 때 빠른 속도로 전체 텍스트 검색이 가능하게 하기 위함이다. 

3) 순위 결정 
순서 결정은 알고리즘을 활용해 특정 검색어에 수많은 문서와 정보들 중 어떤 것들을 먼저 보여주는지에 대한 순서를 정해주는 것이다. 이는 랭킹이라고도 하는데 포털, 전자상거래, 커뮤니티 등 비즈니스 특성에 따라 다양한 정보와 항목들을 결합해 점수화하고 이에 따라 순위를 결정짓게 된다. 
 
순위 결정 알고리즘 중 유명한 것은 페이지 랭크(PageRank, PR) 알고리즘이다. 이는 구글 검색이 검색 엔진 결과에서 웹 페이지의 순위를 매기기 위해 사용한다. 페이지 랭크는 웹사이트 페이지의 중요성을 측정하는 방법이다. 구글에 따르면, 페이지 랭크는 웹사이트의 중요성에 대한 대략적인 추정치를 결정하기 위해 페이지에 대한 링크 수와 품질을 세는 방식으로 작동한다. 근본적인 가정은 더 중요한 웹사이트들이 다른 웹사이트들로부터 더 많은 링크를 받을 가능성이 있다. 

4) 퀴리 프로세싱
검색 상자에 입력하는 쿼리는 숫자로 변환돼야 엔진에서 그 요청을 처리할 수 있다. 그러나 그것이 숫자로 변환되기 전에 검색 엔진은 몇 개의 용어를 없앤다. 대부분의 검색 엔진에는 검색되지 않는 단어와 불용어 조사, 접미사와 같이 문장에는 자주 등장하지만 실제 의미 분석을 하는데 기여하지 않는 단어
 리스트가 있다. 사용자가 검색 상자에 입력하는 쿼리 중 필터링되지 않으면 그 단어를 따옴표로 감싸야 한다.

5-2. 통일 빅데이터센터 자료 검색 엔진의 특수성 
북한과 통일에 관련된 정보를 필요로 하는 사람은 증가하고 있다. 하지만 기존 검색 엔진이나 전문 기관 사이트에서는 북한과 통일에 관한 정보를 얻기 어렵다. 기존 검색 엔의 현황은 크게 포탈 검색 엔진, 논문 위주 학술 자료 검색 엔진 그리고 북한 관련 전문 기관 검색 엔진으로 분류할 수 있다. 
 
구글이나 네이버와 같은 포탈 검색 엔진은 사람들의 선호도가 높은 정보와 다른 사이트와 링크로 많이 연결된 정보의 우선도가 높다는 문제점이 있다. 그렇기 때문에 북한과 통일에 대해 검색할 때 사설 자료가 많이 나오게 된다.
 
RISS나 GoogleScholar와 같은 논문 위주 학술 자료 검색 엔진은 다른 논문으로부터 인용 연결이 많은 정보의 우선도가 높다. 이는 전문 자료는 많으나 북한, 통일 관련 전문 자료의 우선순위가 낮은 문제가 있다.
 
통일부 북한자료센터와 같이 북한 관련 전문 기관 검색 엔진은 키워드 검색만으로는 원하는 결과를 바로 얻지 못하여 카테고리를 여러 번 클릭 해야만 원하는 결과를 얻을 수 있다. 사용자가 카테고리를 잘 모르는 경우 검색이 용이하지 않다는 문제가 있다. 
 
따라서 현 상황에서 북한과 통일에 관한 전문 학술 자료를 얻는 것은 어려운 일이다. 본 통일빅데이터센터 검색 엔진은 오픈 소스 소프트웨어를 이용하여 기존 검색 엔진의 문제점을 해결하고 북한 관련 전문 학술 자료를 제공한다. 
 
구체적으로 북한 연구 자료, 연구 결과물 등을 크롤링 하여 저장하는 데이터베이스를 구축하고 검색 엔진을 통해 인덱싱하여 효율적인 검색과 유지보수가 가능하도록 한다. 또한 연구 자료 및 결과물들로부터 최대한 많은 정보를 데이터화 시켜 검색 결과를 향상시켜준다. 랭킹 알고리즘 또한 연구하여 검색어와 일치하는 검색 결과가 항상 첫 페이지에 제공되도록 한다. 

5-3. 통일 빅데이터센터 자료 검색 시스템 아키텍쳐 
본 절에는 통일빅데이터센터 자료 검색 시스템 아키텍처에 대해 소개한다. 아래 그림 1에서 아키텍처는 데이터 크롤링, 데이터 사일로, 데이터 인덱싱, 쿼리 프로세싱으로 구성된다. 각 파트에 대한 설명과 함께 실제 어떤 도구가 사용되어 검색 시스템이 구현되었는지 상세히 설명한다. 

<그림>


5-3-1. 사이트 크롤링 
통일 빅데이터센터 자료 검색 시스템을 만들기 위해서는 가장 먼저 통일 관련 데이터들을 모아야 한다. 이때 크롤러는 자신이 방문한 페이지에 걸려 있는 모든 링크들까지 확장해서 정보를 모으지 않고 특정 156개의 통일 및 북한 자료 관련 국내 기관, 포털 사이트, 학술 자료 사이트에서만 데이터를 크롤링 해온다. 
 
스크래피 (Scrapy)는 웹페이지에서 구조화된 데이터를 추출하기 위한 오픈 소스 프레임워크이다. 언어는 Python이 지원된다. 스크래피는 기본적으로 JSON, XML, CSV 등의 포맷으로 내보내는 것을 지원하며, HTML 전체 데이터를 가져오기 보다, 특정 데이터를 뽑아내는데 최적화 되어있다. 
 
크롤러는 이미 방문했거나 수집되어 있는 정보를 최신 상태로 유지하기 위해 일정 기간을 정해 지속적으로 재방문해 정보의 변화 여부를 확인하여 최신 상태로 업데이트해야 한다. 장기적인 측면에서 코드를 스파이더를 실행하는 부분과 각 페이지 마다 크롤링 해오는 코드를 모듈화 시켜 분리되도록 설계한다. 이를 통해 확장성을 확보되며 유지 보수가 원활히 진행될 수 있도록 한다. 
 
웹페이지 안에는 텍스트 뿐만 아니라 다양한 형식의 첨부파일이 존재할 수 있다. 그러므로 크롤러는 웹페이지의 텍스트 뿐만 아니라 첨부파일까지 크롤링 해와야 한다. 사용자에게 더 유용한 검색 결과를 보여주기 위해서는 첨부파일 안의 텍스트까지 검색될 수 있게 데이터화 해야 한다. 즉, 첨부파일 안에 있는 텍스트를 파싱해 와야한다. 아파치 티카 (Apache Tiak)툴킷은 PPT, XML, PDF와 같은 천 개 이상의 파일 형식에서 메타데이터와 텍스트를 검색하고 추출하는 오픈 소스 소프트웨어이다. 티카를 사용하여 한글 파일, 워드 파일, PDF 파일 안에 있는 텍스트를 파싱한다. 
 
파일에는 텍스트 뿐만 아니라 그림, 표, 참고 문헌 등 다양한 데이터 타입이 존재한다. 앞으로는 이러하 데이터까지 수집하기 위해 오픈 소스를 이용하여 문서에 삽입된 이미지, 캡션 그리고 테이블에 관한 정보를 파싱 할 것이다. 

5-3-2. 데이터 사일로
크롤링된 데이터는 곧바로 인덱스에 저장되지 않고 데이터 사일로에 먼저 저장된다. 데이터 사일로 (silo)란, 고정된 데이터의 저장소로 농장 사일로의 곡물이 외부 요소로부터 차단되는 것과 마찬가지로 데이터베이스가 한 부서의 통제 하에 있고 조직의 나머지로부터 격리되어 있다. 데이터 사일로는 각 조직 단위의 목표, 우선순위 및 책임이 다를 때 자연스럽게 발생하는 경향이 있다. 
 
본 통일빅데이터센터에서 또한 자료 검색과 텍스트 분석의 목표와 우선순위가 다르기 때문에 데이터 사일로를 사용하여 원본 데이터를 독립적으로 저장한다. 각 목적에 따라 이 원본 데이터를 활용하게 한다. 이를 통해 원본 데이터와 인덱스 안의 데이터를 구분하였고 데이터 분석을 할 때 원본 데이터와 인덱스 안의 데이터를 모두 고려하여 분석하기에 용이한 데이터를 사용할 수 있게 한다. 
 
데이터 사일로는 크로스 플랫폼 도큐먼트 지향 데이터베이스 시스템인 MongoDB를 사용하여 구현된다. NoSQL 데이터 베이스로 분류되는 MongoDB는 JSON과 같은 동적 스키마형 문서들을 선호함에 따라 전통적인 테이블 기반 관계형 데이터 베이스의 구조의 사용을 삼가 한다. 이로써 특정한 종류의 애플리케이션을 더 쉽고 더 빠르게 데이터 통합을 가능케 한다.
 
MongoDB는 컬렉션을 사용해 데이터를 하나로 묶는다. 컬렉션 (collection)이란 용도가 같거나 유사한 문서들을 그룹으로 묶은 것이다. 컬렉션은 기존SQL 데이터 베이스의 테이블처럼 동작한다. 
 
본 통일빅데이터센터의 데이터베이스는 각각의 기관사이트 마다 구현된다. 예를 들어, 그림 4의 kolofoboard, nkdboard, nkoreaboard, nuacboard  데이터베이스는 각각의 사이트에 있는 정보들이 저장되는 곳이다. 각 사이트에는 여러 게시판이 존재한다. 각 사이트 별로 데이터베이스가 만들어지고 여러 게시판들은 하나의 컬렉션으로 묶이게 된다. 또한 그림 4의 attachment는 각 사이트의 첨부파일이 저장되는 데이터베이스이다.
 
MongoDB 에서는 도큐먼트당 그 사이즈가16 MB 로 제한된다. 만약 바이너리 데이터가 다른 도큐먼트의 필드로 추가된다면 16 MB 로 제한돼야 한다. 그때 바이너리 데이터는 BSON 타입으로 임베딩 된다. 하지만 현실적으로 문서는 충분히 16 MB가 초과될 수 있다. MongoDB는 BSON 문서 크기가 16 MB를 초과하는 경우 그 파일을 저장하고 검색할 수 있게 하는 GridFS를 제공한다. GridFS는 파일을 단일 문서에 저장하는 대신 파일을 청크로 나누고 각 청크를 별도의 문서로 저장한다. 
 
GridFS는 파일을 저장하기위해 두 개의 컬렉션을 사용한다. 한 컬렉션은 파일 청크를 저장하고 다른 컬렉션은 파일 메타데이터를 저장한다. 예를 들어, 그림 5에나와있는 attachment 데이터베이스는 fs.chunks와 fs.files 컬렉션을 가지고 있다. 그림 3는  fs.files 컬렉션이며 파일들의 메타데이터가 저장된다. 그림 2는 fs.chunks 컬렉션이며 부모 파일이 여러 개의 청크로 나뉘어져 저장된다. fs.chunks 컬렉션의 필드인 files_id는 부모 파일의 _id 필드이다. 이를 통해 부모 파일을 찾을 수 있다.  

<그림>

 
<그림>


<그림>
 


<그림>

 
문서(document)란 MongoDB 데이터베이스 내에 있는 한 가지 데이터 실체를 나타내는 표현이다. 컬렉션은 한 개 이상의 연관된 실체들로 이뤄져 있다. MongoDB에서 문서들은 내부 하위 문서들을 포함하고 있어서 애플리케이션에 훨씬 더 가까운 고유 데이터 모델을 제공한다. 
 
그림 7은 민주평화통일자문회의사이트의 정책건의보고서 게시판이다. 해당 게시판을 통해 포스트 제목,날짜, 작성자, 바디 내용, 사이트 (기관) 이름, 사이트 URL, 첨부파일 URL, 첨부파일 이름, fs.files 컬렉션에 저장된 첨부파일의 id 그리고 첨부파일에서 파싱해온 텍스트 내용을 저장한다. 그림 6을 통해 위에서 말한 정보들이 구체적으로 어떻게 저장되고 있는지 확인할 수 있다. 이는  nuacboard 컬렉션의 문서 중 일부이다.


<그림>



<그림>

 
데이터 사일로에 있는 데이터들은 검색 엔진 인덱스로 옮겨져야 한다. 그때 그 사이에 동기화를 맞추기 위해 Transporter 한 스토어에서 다른 스토어로 데이터를 전송해주는 오픈 소스 소프트웨어이다 (https://github.com/compose/transporter).
가 사용된다.
 
5-3-3. 문서 인덱싱
Transporter를 통해 인덱스에 옮겨진 데이터는 검색 엔진을 통해 인덱싱 된다. 현재 검색 엔진은 여러 개의 오픈 소스 라이브러리들을 통해 개발할 수 있다.

5-3-3-1. 다양한 검색 엔진 오픈 소스 라이브러리
1) 아파치 루씬 코어 (Apache Lucene Core)
루씬은 Java를 기반으로 하며 아파치 라이센스 하에 배포되는 가장 신뢰할 수 있는 교차 플랫폼 오픈 소스 검색 엔진 프로젝트이다.루씬은 순수 Java로만 쓰여졌음에도 불구하고, Delphi,Perl, C#, C++, Python, Ruby, 그리고PHP 와 같은 다른 프로그래밍 언어로도 이용 가능하다. 루씬은 가장 좋은 검색 결과를 처음으로 반환하는 랭킹 검색 시스템이다. 

2) 엘리스틱서치 (Elasticsearch)
엘라스틱서치는 아파치 루씬을 기반으로 한 분산된 RESTful한 검색 및 분석 엔진을 제공하는 오픈 소스 검색 엔진 소프트웨어이다. 엘라스틱서치는 중소기업에서 대기업까지 지원할 수 있는 확장성이 높은 오픈 소스 검색 엔진이다. 엘라스틱서치는 HTTP 웹 인터페이스와Schema-free JSON 문서로 전체 텍스트 검색 기능을 제공한다. 

3) 아파치 솔라 (Apache Solr)
엘라스틱서치는 이후 아파치 솔라는 또 다른 인기 오픈 소스 검색 엔진 소프트웨어이다. 솔라는 역시 자바로 개발되었고 풀-텍스트 검색과 실시간 인덱싱을 지원한다. 게다가, 엘라스틱서치와 마찬가지로 아파치 솔라도 루씬을 기반으로 하며 자바 검색 라이브러리를 사용한다. HTTP를 통해JSON, XML, CSV 또는 바이너리 파일을 통해 솔라에서 인덱싱을 수행할 수 있다. 그리고 HTTP GET를 사용하여 검색 결과를 조회한다.
 
솔라는 안정화된 오픈 소스이며 장문 데이터 검색에 용이하다는 장점이 있다. 하지만 엘라스틱서치는 실시간 인덱싱이 가능하고 검색, 인덱싱 속도가 상대적으로 빠르다. 물론 엘라스틱서치에서 장문 데이터 검색 시 속도가 저하된다는 단점이 있다. 하지만 보통 검색을 할 때 장문 데이터 검색을 잘 하지 않기 때문에 본 통일빅데이터센터의 검색 엔진으로 현재 상승세인 엘라스틱서치를 선택한다. 
 
5-3-3-2. 엘라스틱서치
엘라스틱서치는 클러스터, 노드, 인덱스, 도큐먼트, 샤드 그리고 레플리카로 구성된다. 
 
클러스터는 모든 데이터를 함께 가지고 있는 한 개 또는 그 이상의 노드의 집합이다. 클러스터는 연합된 인덱싱과 모든 노드를 검색할 수 있는 기능을 제공한다. 클러스터는 고유한 이름으로 식별된다.
 
노드는 클러스터의 일부로 단일 서버이다. 노드는 데이터를 보관하고 클러스터 인덱싱과 검색 능력에 관여한다.
 
인덱스는 비슷한 특성을 가진 도큐먼트의 집합이다. 인덱스는 이름으로 구분된다. 이 이름은 인덱싱, 검색, 업데이트 그리고 삭제를 수행하는 동안 인덱스를 참조하기 위해 사용된다. 삭제 명령은 인덱스 내부의 도큐먼트에 대해서도 이루어 질 수 있다. 인덱스는 단일 클러스터에서 사용자가 원하는 만큼 많이 정의된다.
 
도큐먼트는 인덱싱될 수 있는 정보의 기본 단위이다. 도큐먼트는 유비쿼터스 인터넷 교환 포맷인 JSON으로 표현된다. 
 
샤드는 인덱스가 여러 조각으로 나뉜 것이다. 엘라스틱서치는 인덱스를 샤드로 나눌 수 있는 기능을 제공한다. 각각의 샤드는 그 자체의 내부에서 완전히 기능하며 클러스터 내부의 어떤 노드에서도 관리될 수 있는 독립된 인덱스이다. 인덱스가 단일 노드에 들어가면 단일 노드 내의 디스크 사용 가능 공간보다 더 많은 공간을 차지할 때 샤드가 유용하다. 다른 노드들 사이에 인덱스는 그 때 다시 또 한 번 나누어진다. 샤드를 통해 명령을 분배하고 병렬적으로 처리하게 됨으로 검색 엔진의 성능을 올려준다.
 
레플리카는 레플리카 샤드라고도 불린다. 이는 인덱스의 샤드에 대한 한 개 이상의 복사본이다. 레플리카를 통해 노드가 깨졌을 때를 대비하여 높은 가용성을 제공한다. 또한 레플리카를 통해 검색이 모든 레플리카에서 병렬적으로 실행될 수 있다. 이를 통해 볼륨의 스케일 업을 해준다. 
 
5-3-3-3. 노리 (Nori) 형태소 분석기
데이터들은 인덱싱이 되기 전에 형태소 분석기를 통해 전처리 과정을 거치게 된다. 하지만 한글은 다른 언어와 달리 조사나 어미의 접미사가 명사, 동사 등과 결합하기 때문에 기본 형태소 분석기로는 분석하기 어렵다. 그렇기 때문에 검색 엔진을 한글에 적용하기 위해서는 별도의 한글 형태소 분석기가 필요하다. 노리 형태소 분석기는 루씬 프로젝트에서 공식적으로 제공되는 한글 형태소 분석기로써 엘라스틱서치 6.4 버전에서 공식적으로 배포되었다. 내부적으로 세종 말뭉치와 mecab-ko-dic 사전을 사용하여 기존 형태소 분석기와는 다르게 사전을 모두 압축하여 사용한다. 
 
노리 형태소 분석기는 하나의 토크나이저와 두 개의 토큰 필터 (nori_part_of_speech, nori_readingform)로 구성된다. nori_part_of_speech 토큰 필터는 품사 태그 세트와 일치하는 토큰을 찾아 제거하는 토큰 필터이다. 즉, 모든 명사를 역색인으로 생성하는 것이 아니라, 역색인 문서를 생성하기 전에 명시한 품사 태그와 일치하는 토큰을 제거하여 역색인을 생성한다. 이러한 삭제할 품사 태그는 stoptags라는 파라미터를 이용하여 지정한다. nori_readingform 토큰 필터는 사용자 질의에 존재하는 한자를 한글로 변경하는 역할을 하는 필터이다. 
 
본 통일빅데이터센터 검색 엔진에는 두개의 토큰 필터 모두 사용된다. nori_part_of_speech 토큰 필터를 통해 E (어미), IC (감탄사), J (조사), MAG (일반 부사), MAJ (접속 부사), MM (관형사), NA (알려지지 않은), SC (구분 기호), SE (생략), SF (마침표,물음표, 느낌표), SP (띄어쓰기), SSC (닫는 괄호), SSO (여는 괄호), SY (기타 가호), UNA (알 수 없는), UNKNOWN (알 수 없는), VSV (알 수 없는), XPN (접두사), XSA (형용사 접미사), XSN (명사 접미사) 그리고 XSV(동사 접미사) 태그가 불용어로 처리된다. 또한 통일 및 북한 관련 문헌에 한자가 많이 나오기 때문에 nori_readingform 토큰 필터를 통해 한글로 변경한다. 
 
5-3-4. 쿼리 프로세싱   
사용자가 검색 박스에 쿼리를 입력하면 그 쿼리는쿼리 프로세싱을 거친다. 쿼리 프로세싱은 검색 결과를 가져오는 프로세스이다. 
 
엘라스틱서치는 term,match, multi match, bool, fuzzy 등 다양한 쿼리를 지원한다. 
1) term 쿼리
term 쿼리는 term 옵션을 사용하여 쿼리문의 저장된 term과 정확히 일치하는 내용을 찾는다. 예를 들어, 그림 8은 post_body 필드에서 "남북" 키워드와 정확히 일치하는 문서들을 보여줄 것이다. 

<그림>


2) terms 쿼리
terms 쿼리는 terms 옵션을 사용하여 2개 이상의 terms을 같이 검색한다. terms 쿼리는 항상 배열 형식으로 입력한다. minimum_shuld_match옵션으로 최소 일치 수도 지정 가능하다. 예를 들어, 그림 9는 post_body 필드에서 "남북" 또는 "통일"키워드와 정확히 일치하는 문서들을 보여줄 것이다. 하지만 minimum_shuld_match 옵션 값이 2이므로 두 개의 키워드 모두 일치되는 문서들을 보여줄 것이다. 

<그림>

 
2) match,multi match 쿼리
match 쿼리는 그림 10처럼 match 옵션을 사용한다. 쿼리문 또한 형태소 분석을 거친 뒤 사용한다. multi_match 옵션을 사용하면 여러 필드에서 검색 가능하다. 

<그림>


3) bool 쿼리
bool 쿼리는 내부의 쿼리로 다른 쿼리에 포함시켜 사용한다. 쿼리 조건문인 bool 조합으로 적용하여 최종 검색 결과를 나타낸다. 

4) fuzzy 쿼리
fuzzy 검색은 입력된 검색 키워드가 정확하지 않아도 사용자의 요구를 예상하고 적절한 단어를찾는 검색 방식으로 표기의 흔들림과 유의어동의어를 보완한다. 엘라스틱서치에서는 편집거리 알고리즘을 기반으로 유사 단어 검색을 지원한다. 
 
엘라스틱서치에서 문서의 점수 값을 계산하는 것을 정확도 (relevance)라고 한다. 즉, 검색의 정확도를 뜻하며 기본적으로 relevance 알고리즘을 사용한다. 엘라스틱서치에서는 문서 (필드)안에 term이 자주 출현할 수록, 인덱싱된 모든 문서 (필드)에 term이 적게 존재할 수록, 문서 (필드)가 짧을수록 고득점을 얻게 된다. 
 
그림 11은 post_body 필드에 "남북" 키워드를 검색하는 소스코드이다. 그 결과 그림 12에서 볼 수 있듯이 post_body 필드에 "남북" 키워드가 일치되는 문서는 총 17개 이며 max_score는 5.4279046점이다. 

<그림>
 

<그림>

 
 
5-4. 프로토타입
본 통일빅데이터센터 검색 엔진의 스펙은 다음과 같다.

1) 서버 스펙
-OS 정보: CentOS Linuxrelease 7.6.1810 (Core)
-CPU 모델명: Intel(R)Core(TM)i7-8700 CPU @ 3.20GHz
-CPU 당 물리 코어 수: 6
-물리 CPU 수: 1
-리눅스 전체 코어 개수: 12
-메모리 개수: 8192 MBX2ea
-총 메모리 사이즈: 15911496 kB
 
2) 소프트웨어 스펙
-Elasticsearch: Version: 7.3.1
-Java: openjdk version "1.8.0_222"
-MongoDB: db version v4.0.12
 
그림 13은 본 통일빅데이터센터 검색 엔진의 인덱스 리스트이다. nkdboard, kolofoboard, nkoreaboard, nuacboard 의 이름을 가진 인덱스가 존재하는 것을 볼 수 있다.

<그림>

그림 14는 집계실행시키는 소스코드이다. 그림 15는 첨부파일의 텍스트 정보가 저장된 "file_extracted_content"필드에서 많이 나온 단어를 집계한 결과이다. 민주, 통일, 평화, 건의, 자문, 회의공 공감, 국민 등과 같은 단어들이 집계되었다. 

<그림>


5-5. 자료 검색 시스템 구축 현황
크롤링 현황은 156개 중 11개의 사이트가 크롤링 되었다. 인덱싱은 156개 중 9개의 사이트가 인덱싱 되었다. 그림 13에서 볼 수 있듯이 nkdboard 인덱스 사이즈는 740.5kb, nkoreaboard 인덱스 사이즈는 4mb, kolofoboard 인덱스 사이즈는 7.6mb 그리고nuacboard 인덱스 사이즈는 3.3mb이다. 

Ⅶ. 참고 자료
-http://web.skhu.ac.kr/~mckim1/Lecture/IR/Note/hwork.html
-https://opencompu.tistory.com/entry/인터넷-검색-엔진이란
-http://www.seo-korea.com/검색엔진의-원리/
-https://brunch.co.kr/@highfree/25
-https://en.wikipedia.org/wiki/Search_engine_indexing
-https://en.wikipedia.org/wiki/Inverted_index
-https://en.wikipedia.org/wiki/PageRank
-https://lifepacific.libguides.com/c.php?g=155121
-https://docs.scrapy.org/en/latest/
-https://searchdatamanagement.techtarget.com/definition/data-silo
-https://docs.mongodb.com/manual/core/gridfs/
-https://www.how2shout.com/tools/top-best-open-source-search-engine-software-enterprises.html
-https://medium.com/@victorsmelopoa/an-introduction-to-elasticsearch-with-kibana-78071db3704
-https://coding-start.tistory.com/167
-https://www.elastic.co/guide/en/elasticsearch/reference/7.5/query-dsl.html
